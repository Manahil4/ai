{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/5\n",
      "Best fitness (val_loss): 0.09121539443731308\n",
      "Best accuracy (val_accuracy): 0.9747999906539917\n",
      "Generation 2/5\n",
      "Best fitness (val_loss): 0.08174171298742294\n",
      "Best accuracy (val_accuracy): 0.9765999913215637\n",
      "Generation 3/5\n",
      "Best fitness (val_loss): 0.07699450850486755\n",
      "Best accuracy (val_accuracy): 0.9800000190734863\n",
      "Generation 4/5\n",
      "Best fitness (val_loss): 0.08287367224693298\n",
      "Best accuracy (val_accuracy): 0.9783999919891357\n",
      "Generation 5/5\n",
      "Best fitness (val_loss): 0.0943819060921669\n",
      "Best accuracy (val_accuracy): 0.9724000096321106\n",
      "Best Hyperparameters: {'neurons1': 135, 'neurons2': 204, 'learning_rate': 0.0009977066121945018}\n",
      "Best Validation Loss: 0.0943819060921669\n",
      "Best Validation Accuracy: 0.9715999960899353\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "# Ensure the data is reshaped correctly\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and split the data\n",
    "X_train_full, X_test = X_train_full / 255.0, X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "X_valid = X_valid.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "\n",
    "\n",
    "# Step 1: Define search space\n",
    "def initialize_population(pop_size):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        individual = {\n",
    "            \"neurons1\": random.randint(32, 512),\n",
    "            \"neurons2\": random.randint(32, 512),\n",
    "            \"learning_rate\": 10 ** random.uniform(-4, -2)  # Log-scale\n",
    "        }\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "# Step 2: Create the model\n",
    "def create_model(neurons1, neurons2, lr):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),  # Adjust input shape for MNIST\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(neurons1, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(neurons2, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")  # 10 classes for digit classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Step 3: Fitness function\n",
    "\n",
    "def fitness(individual, X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "\n",
    "    model = create_model(individual[\"neurons1\"], individual[\"neurons2\"], individual[\"learning_rate\"])\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid),\n",
    "                    batch_size=32, verbose=0, callbacks=[early_stopping])\n",
    "    # history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid), batch_size=32, verbose=0)\n",
    "    val_loss = history.history[\"val_loss\"][-1]  # Use final validation loss as fitness\n",
    "    val_accuracy = history.history[\"val_accuracy\"][-1]  # Final validation accuracy\n",
    "    return val_loss, val_accuracy\n",
    "    \n",
    "# Step 4: Selection (Tournament Selection)\n",
    "def select_parents(population, fitness_scores):\n",
    "    parents = random.choices(population, weights=1/np.array(fitness_scores), k=2)  # Lower loss = higher fitness\n",
    "    return parents\n",
    "\n",
    "# Step 5: Crossover\n",
    "def crossover(parent1, parent2):\n",
    "    child = {\n",
    "        \"neurons1\": random.choice([parent1[\"neurons1\"], parent2[\"neurons1\"]]),\n",
    "        \"neurons2\": random.choice([parent1[\"neurons2\"], parent2[\"neurons2\"]]),\n",
    "        \"learning_rate\": random.choice([parent1[\"learning_rate\"], parent2[\"learning_rate\"]])\n",
    "    }\n",
    "    return child\n",
    "\n",
    "# Step 6: Mutation\n",
    "def mutate(individual, mutation_rate=0.1):\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons1\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons2\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"learning_rate\"] = 10 ** random.uniform(-4, -2)\n",
    "    return individual\n",
    "\n",
    "# Step 7: Genetic Algorithm\n",
    "def genetic_algorithm(X_train, y_train, X_valid, y_valid, generations, pop_size, mutation_rate):\n",
    "    population = initialize_population(pop_size)\n",
    "    for generation in range(generations):\n",
    "        print(f\"Generation {generation+1}/{generations}\")\n",
    "        \n",
    "        # Evaluate fitness\n",
    "        results = [fitness(ind, X_train, y_train, X_valid, y_valid) for ind in population]\n",
    "        fitness_scores = [result[0] for result in results]  # Extract val_loss\n",
    "        accuracy_scores = [result[1] for result in results]  # Extract val_accuracy\n",
    "        \n",
    "        # Log the best results of the current generation\n",
    "        best_loss = min(fitness_scores)\n",
    "        best_accuracy = max(accuracy_scores)\n",
    "        print(f\"Best fitness (val_loss): {best_loss}\")\n",
    "        print(f\"Best accuracy (val_accuracy): {best_accuracy}\")\n",
    "        \n",
    "        # Select next generation\n",
    "        new_population = []\n",
    "        for _ in range(pop_size // 2):  # Each iteration produces 2 children\n",
    "            parent1, parent2 = select_parents(population, fitness_scores)\n",
    "            child1 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            child2 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            new_population.extend([child1, child2])\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # Return the best hyperparameters\n",
    "    best_index = np.argmin(fitness_scores)\n",
    "    # print(f\"Final Best Accuracy (val_accuracy): {accuracy_scores[best_index]}\")\n",
    "    return population[best_index], best_loss, accuracy_scores[best_index]\n",
    "\n",
    "#  Run Genetic Algorithm\n",
    "best_hyperparameters, best_loss, best_accuracy = genetic_algorithm(\n",
    "    X_train, y_train, X_valid, y_valid, \n",
    "    generations=5, \n",
    "    pop_size=5, \n",
    "    mutation_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(f\"Best Validation Loss: {best_loss}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\intel\\Desktop\\ai cep\\myenv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fitness (val_loss): 0.22106555104255676\n",
      "Best accuracy (val_accuracy): 0.9732000231742859\n",
      "Generation 2/5\n",
      "Best fitness (val_loss): 0.18981164693832397\n",
      "Best accuracy (val_accuracy): 0.9751999974250793\n",
      "Generation 3/5\n",
      "Best fitness (val_loss): 0.23137754201889038\n",
      "Best accuracy (val_accuracy): 0.9718000292778015\n",
      "Generation 4/5\n",
      "Best fitness (val_loss): 0.22234196960926056\n",
      "Best accuracy (val_accuracy): 0.9696000218391418\n",
      "Generation 5/5\n",
      "Best fitness (val_loss): 0.2094084918498993\n",
      "Best accuracy (val_accuracy): 0.9742000102996826\n",
      "Best Hyperparameters: ({'neurons1': 466, 'neurons2': 302, 'learning_rate': 0.00022667188523287847}, [0.22106555104255676, 0.18981164693832397, 0.23137754201889038, 0.22234196960926056, 0.2094084918498993], [0.9732000231742859, 0.9751999974250793, 0.9718000292778015, 0.9696000218391418, 0.9742000102996826])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_hyperparameters)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Train the best model on the full training set (train + valid) and evaluate on test set\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m final_model \u001b[38;5;241m=\u001b[39m create_model(\u001b[43mbest_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneurons1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[0;32m    146\u001b[0m best_hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneurons2\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m    147\u001b[0m best_hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    149\u001b[0m final_model\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39mconcatenate([X_train, X_valid]), \n\u001b[0;32m    150\u001b[0m                 np\u001b[38;5;241m.\u001b[39mconcatenate([y_train, y_valid]), \n\u001b[0;32m    151\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m    152\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "# Ensure the data is reshaped correctly\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and split the data\n",
    "X_train_full, X_test = X_train_full / 255.0, X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "X_valid = X_valid.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "\n",
    "# Step 1: Define search space\n",
    "def initialize_population(pop_size):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        individual = {\n",
    "            \"neurons1\": random.randint(32, 512),\n",
    "            \"neurons2\": random.randint(32, 512),\n",
    "            \"learning_rate\": 10 ** random.uniform(-4, -2)  # Log-scale\n",
    "        }\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "# Updated model creation for GA\n",
    "def create_model(neurons1, neurons2, lr):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),  # Input layer for MNIST\n",
    "        \n",
    "        # First hidden layer with BatchNorm, He init, and L2 regularization\n",
    "        keras.layers.Dense(neurons1, kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01), use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        \n",
    "        # Second hidden layer\n",
    "        keras.layers.Dense(neurons2, kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01), use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        keras.layers.Dropout(rate=0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        keras.layers.Dense(10, activation=\"softmax\", use_bias=False)\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "loss=\"sparse_categorical_crossentropy\", \n",
    "metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def fitness(individual, X_train, y_train, X_valid, y_valid):\n",
    "    model = create_model(individual[\"neurons1\"], individual[\"neurons2\"], individual[\"learning_rate\"])\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=5, \n",
    "        validation_data=(X_valid, y_valid),\n",
    "        batch_size=32, \n",
    "        verbose=0, \n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    val_loss = history.history[\"val_loss\"][-1]  # Final validation loss\n",
    "    val_accuracy = history.history[\"val_accuracy\"][-1]  # Final validation accuracy\n",
    "    \n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Step 4: Selection (Tournament Selection)\n",
    "def select_parents(population, fitness_scores):\n",
    "    parents = random.choices(population, weights=1/np.array(fitness_scores), k=2)  # Lower loss = higher fitness\n",
    "    return parents\n",
    "\n",
    "# Step 5: Crossover\n",
    "def crossover(parent1, parent2):\n",
    "    child = {\n",
    "        \"neurons1\": random.choice([parent1[\"neurons1\"], parent2[\"neurons1\"]]),\n",
    "        \"neurons2\": random.choice([parent1[\"neurons2\"], parent2[\"neurons2\"]]),\n",
    "        \"learning_rate\": random.choice([parent1[\"learning_rate\"], parent2[\"learning_rate\"]])\n",
    "    }\n",
    "    return child\n",
    "\n",
    "# Step 6: Mutation\n",
    "def mutate(individual, mutation_rate=0.1):\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons1\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons2\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"learning_rate\"] = 10 ** random.uniform(-4, -2)\n",
    "    return individual\n",
    "\n",
    "\n",
    "def genetic_algorithm(X_train, y_train, X_valid, y_valid, generations=10, pop_size=10, mutation_rate=0.1):\n",
    "    population = initialize_population(pop_size)\n",
    "    all_fitness_scores = []  # To store all generations' fitness scores\n",
    "    all_accuracy_scores = []  # To store all generations' accuracy scores\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"Generation {generation+1}/{generations}\")\n",
    "        \n",
    "        # Evaluate fitness for each individual\n",
    "        fitness_scores = []\n",
    "        accuracy_scores = []\n",
    "        for ind in population:\n",
    "            val_loss, val_accuracy = fitness(ind, X_train, y_train, X_valid, y_valid)\n",
    "            fitness_scores.append(val_loss)\n",
    "            accuracy_scores.append(val_accuracy)\n",
    "        \n",
    "        # Log best results of the current generation\n",
    "        best_loss = min(fitness_scores)\n",
    "        best_accuracy = max(accuracy_scores)\n",
    "        print(f\"Best fitness (val_loss): {best_loss}\")\n",
    "        print(f\"Best accuracy (val_accuracy): {best_accuracy}\")\n",
    "\n",
    "        # Store generation metrics\n",
    "        all_fitness_scores.append(best_loss)\n",
    "        all_accuracy_scores.append(best_accuracy)\n",
    "\n",
    "        # Select next generation\n",
    "        new_population = []\n",
    "        for _ in range(pop_size // 2):  # Each iteration produces 2 children\n",
    "            parent1, parent2 = select_parents(population, fitness_scores)\n",
    "            child1 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            child2 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            new_population.extend([child1, child2])\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # Return the best hyperparameters and their performance\n",
    "    best_index = np.argmin(fitness_scores)\n",
    "    return population[best_index], all_fitness_scores, all_accuracy_scores\n",
    "# Rebuild and evaluate the best model\n",
    "best_hyperparameters = genetic_algorithm(X_train, y_train, X_valid, y_valid, generations=5, pop_size=5, mutation_rate=0.1)\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Train the best model on the full training set (train + valid) and evaluate on test set\n",
    "final_model = create_model(best_hyperparameters[\"neurons1\"], \n",
    "best_hyperparameters[\"neurons2\"], \n",
    "best_hyperparameters[\"learning_rate\"])\n",
    "\n",
    "final_model.fit(np.concatenate([X_train, X_valid]), \n",
    "                np.concatenate([y_train, y_valid]), \n",
    "                epochs=10, \n",
    "                batch_size=32)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = final_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/5\n",
      "Best fitness (val_loss): 0.08322973549365997\n",
      "Best accuracy (val_accuracy): 0.9750000238418579\n",
      "Generation 2/5\n",
      "Best fitness (val_loss): 0.08720405399799347\n",
      "Best accuracy (val_accuracy): 0.9753999710083008\n",
      "Generation 3/5\n",
      "Best fitness (val_loss): 0.0769028589129448\n",
      "Best accuracy (val_accuracy): 0.9764000177383423\n",
      "Generation 4/5\n",
      "Best fitness (val_loss): 0.08446028083562851\n",
      "Best accuracy (val_accuracy): 0.9775999784469604\n",
      "Generation 5/5\n",
      "Best fitness (val_loss): 0.07584147900342941\n",
      "Best accuracy (val_accuracy): 0.9775999784469604\n",
      "Best Hyperparameters: {'neurons1': 472, 'neurons2': 101, 'learning_rate': 0.00041933307406293475}\n",
      "Best Validation Loss: 0.07584147900342941\n",
      "Best Validation Accuracy: 0.9775999784469604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ensure the data is reshaped correctly\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and split the data\n",
    "X_train_full, X_test = X_train_full / 255.0, X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "X_valid = X_valid.reshape(-1, 28, 28)  # Reshape if necessary\n",
    "\n",
    "\n",
    "# Step 1: Define search space\n",
    "def initialize_population(pop_size):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        individual = {\n",
    "            \"neurons1\": random.randint(32, 512),\n",
    "            \"neurons2\": random.randint(32, 512),\n",
    "            \"learning_rate\": 10 ** random.uniform(-4, -2)  # Log-scale\n",
    "        }\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "# Step 2: Create the model\n",
    "def create_model(neurons1, neurons2, lr):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),  # Adjust input shape for MNIST\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(neurons1, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(neurons2, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        keras.layers.Dropout(rate=0.1),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")  # 10 classes for digit classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Step 3: Fitness function\n",
    "def fitness(individual, X_train, y_train, X_valid, y_valid):\n",
    "    model = create_model(individual[\"neurons1\"], individual[\"neurons2\"], individual[\"learning_rate\"])\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid),\n",
    "                        batch_size=32, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    val_loss = history.history[\"val_loss\"][-1]  # Final validation loss\n",
    "    val_accuracy = history.history[\"val_accuracy\"][-1]  # Final validation accuracy\n",
    "    \n",
    "    return val_loss, val_accuracy  # Both loss and accuracy returned\n",
    "\n",
    "# Step 4: Selection (Tournament Selection)\n",
    "def select_parents(population, fitness_scores):\n",
    "    # Combine loss and accuracy for a balanced selection process\n",
    "    combined_scores = np.array(fitness_scores)  # Use loss as the main metric for selection\n",
    "    parents = random.choices(population, weights=1 / combined_scores, k=2)\n",
    "    return parents\n",
    "\n",
    "# Step 5: Crossover\n",
    "def crossover(parent1, parent2):\n",
    "    child = {\n",
    "        \"neurons1\": random.choice([parent1[\"neurons1\"], parent2[\"neurons1\"]]),\n",
    "        \"neurons2\": random.choice([parent1[\"neurons2\"], parent2[\"neurons2\"]]),\n",
    "        \"learning_rate\": random.choice([parent1[\"learning_rate\"], parent2[\"learning_rate\"]])\n",
    "    }\n",
    "    return child\n",
    "\n",
    "# Step 6: Mutation\n",
    "def mutate(individual, mutation_rate=0.1):\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons1\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"neurons2\"] = random.randint(32, 512)\n",
    "    if random.random() < mutation_rate:\n",
    "        individual[\"learning_rate\"] = 10 ** random.uniform(-4, -2)\n",
    "    return individual\n",
    "\n",
    "# Step 7: Genetic Algorithm\n",
    "def genetic_algorithm(X_train, y_train, X_valid, y_valid, generations, pop_size, mutation_rate):\n",
    "    population = initialize_population(pop_size)\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        print(f\"Generation {generation + 1}/{generations}\")\n",
    "        \n",
    "        # Evaluate fitness for each individual in the population\n",
    "        results = [fitness(ind, X_train, y_train, X_valid, y_valid) for ind in population]\n",
    "        fitness_scores = [result[0] for result in results]  # Extract val_loss\n",
    "        accuracy_scores = [result[1] for result in results]  # Extract val_accuracy\n",
    "        \n",
    "        # Log the best results of the current generation\n",
    "        best_loss = min(fitness_scores)\n",
    "        best_accuracy = max(accuracy_scores)\n",
    "        print(f\"Best fitness (val_loss): {best_loss}\")\n",
    "        print(f\"Best accuracy (val_accuracy): {best_accuracy}\")\n",
    "        \n",
    "        # Select next generation\n",
    "        new_population = []\n",
    "        for _ in range(pop_size // 2):  # Each iteration produces 2 children\n",
    "            parent1, parent2 = select_parents(population, fitness_scores)\n",
    "            child1 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            child2 = mutate(crossover(parent1, parent2), mutation_rate)\n",
    "            new_population.extend([child1, child2])\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    # Return the best hyperparameters, loss, and accuracy\n",
    "    best_index = np.argmin(fitness_scores)  # Best individual based on validation loss\n",
    "    return population[best_index], best_loss, accuracy_scores[best_index]\n",
    "\n",
    "# Run Genetic Algorithm\n",
    "best_hyperparameters, best_loss, best_accuracy = genetic_algorithm(\n",
    "    X_train, y_train, X_valid, y_valid, \n",
    "    generations=5, \n",
    "    pop_size=5, \n",
    "    mutation_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(f\"Best Validation Loss: {best_loss}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
